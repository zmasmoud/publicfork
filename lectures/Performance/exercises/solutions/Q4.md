# Proposed Solution

## A baseline benchmark

We create a `Benchmarks.java` file, and add the following code:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarks {

    @Benchmark
    public void noCacheBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt");
        for (int i = 0; i < 5; ++i) {
            bh.consume(reader.read(10));
        }
    }
}
```

Notice that the benchmark is using `Blackhole` to consume the results. This
is required to avoid the compiler from optimizing the code away.

## Caching the reads

We can now create a `Cache` implementation. We will use a `HashMap` to store
the values. We will also create a `UselessCache` implementation that does not
cache anything.

```java
public class StudentCache implements Cache<Integer, Student> {

    private Map<Integer, Student> kvStore;

    public StudentCache() {
        kvStore = new HashMap<>();
    }

    @Override
    public boolean contains(Integer key) {
        return kvStore.containsKey(key);
    }

    @Override
    public void put(Integer key, Student value) {
        kvStore.put(key, value);
    }

    @Override
    public void clear() {
        kvStore = new HashMap<>();
    }

    @Override
    public Student get(Integer key) {
        return kvStore.get(key);
    }
}
```

```java
public class UselessCache<K, V> implements Cache<K, V> {

    @Override
    public boolean contains(K key) {
        return false;
    }

    @Override
    public void put(K key, V value) {
    }

    @Override
    public V get(K key) {
        return null;
    }

    @Override
    public void clear() {
    }
}
```

We can now update our `CSVReader` to use a `Cache`:

```java
public class CSVReader {

    private final Cache<Integer, Student> cache;
    private final Path path;

    public CSVReader(String fileName, Cache<Integer, Student> cache) {
        this.cache = cache;
        this.path = Paths.get(fileName);
    }

    public List<Student> read(int n) {
        List<Student> result = new ArrayList<>();

        for (int i = 1; i <= n; ++i) {
            if (cache.contains(i)) {
                result.add(cache.get(i));
            }
        }
        if (result.size() == n) return result;

        try (Stream<String> stream = Files.lines(path)) {
            stream.skip(1).limit(n).forEach(line -> {
                if (!line.isEmpty()) {
                    String[] values = line.split("\t");
                    Student s = new Student(values[1], values[2], values[3], values[4]);
                    result.add(s);
                    cache.put(Integer.valueOf(values[0]), s);
                }
            });
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    }
}
```

And update our benchmarks to use the cache:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarking {
    @Benchmark
    public void cacheBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new StudentCache());
        for (int i = 0; i < 5; ++i) {
            bh.consume(reader.read(10));
        }
    }

    @Benchmark
    public void noCacheBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        for (int i = 0; i < 5; ++i) {
            bh.consume(reader.read(10));
        }
    }
}
```

## Processing the reads: sequential vs. parallel

Let's update our benchmarks to use parallel streams, replicating the students list 1000 times to have enough items to observe timing differences:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarking {

    // Ignored for brevity...

    @Benchmark
    public void sequentialBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = new ArrayList<>(reader.read(1000));
        for (int i = 0; i < 1000; ++i) {
            students.addAll(reader.read(1000));
        }
        bh.consume(students.stream().map(Student::getEmail).collect(Collectors.toList()));
    }

    @Benchmark
    public void parallelBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = new ArrayList<>(reader.read(1000));
        for (int i = 0; i < 1000; ++i) {
            students.addAll(reader.read(1000));
        }
        bh.consume(students.stream().parallel().map(Student::getEmail).collect(Collectors.toList()));
    }
}
```

## Storing the reads: Set vs. List

We can now update our benchmarks to use a `Set` instead of a `List`:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarking {

    // Ignored for brevity...

    @Benchmark
    public void listBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = reader.read(1000);
        for (int i = 0; i < 1000; ++i) {
            bh.consume(students.contains(new Student("Adlai", "Bodicum", "abodicumom@cpanel.net", "Konyang University")));
        }
    }

    @Benchmark
    public void setBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        Set<Student> students = new HashSet<>(reader.read(1000));
        for (int i = 0; i < 1000; ++i) {
            bh.consume(students.contains(new Student("Adlai", "Bodicum", "abodicumom@cpanel.net", "Konyang University")));
        }
    }
}
```

## LinkedLists vs. Iterator

Let's compare the performance of iterating over a `LinkedList` manually vs.
using the dedicated `Iterator`:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarking {
    @Benchmark
    public void iterateLinkedListGetBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = new LinkedList<>(reader.read(1000));
        for (int i = 0; i < 1000; ++i) {
            bh.consume(students.get(i));
        }
    }

    @Benchmark
    public void iterateLinkedListForeachBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = new LinkedList<>(reader.read(1000));
        for (final Student s : students) {
            bh.consume(s);
        }
    }
}
```

We notice that iterating over a `LinkedList` using a the `Iterator` is much
faster than using the `get` method. This is because the `get` method has to
iterate over the list until it reaches the desired index, while the `Iterator`
has a pointer to the current element and traverses the list only once.

## ArrayList vs. Iterator

Let's add the dedicated benchmarks to compare the performance of iterating
over an `ArrayList` manually to using the dedicated `Iterator`:

```java

@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(2)
@State(Scope.Benchmark)
public class Benchmarking {

    // Ignored for brevity...

    @Benchmark
    public void iterateArrayListGetBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = reader.read(1000);
        for (int i = 0; i < 1000; ++i) {
            bh.consume(students.get(i));
        }
    }

    @Benchmark
    public void iterateArrayListForeachBenchmark(Blackhole bh) {
        CSVReader reader = new CSVReader("res/students.txt", new UselessCache<>());
        List<Student> students = reader.read(1000);
        for (final Student s : students) {
            bh.consume(s);
        }
    }
}
```

Surprisingly, iterating using an index is faster than using the `Iterator`! Why
is that? First, unlike the `LinkedList`, the `ArrayList` is backed by an array,
so the `get` method is a simple array access. Secondly, the
`for (final Student s : students)` code is actually translated to the following
code:

```java
Iterator<Student> iterator=students.iterator();
while(iterator.hasNext()){
    Student s=iterator.next();
    // Body of the loop...
}
```

This means that the `Iterator` is created and the `hasNext` method is called
for each iteration. This is not the case when using an index, and the usage
of the `Iterator` incurs a small overhead.
